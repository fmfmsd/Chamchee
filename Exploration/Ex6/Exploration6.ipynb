{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ex6. 작사가 인공지능 만들기\n",
        "\n",
        "---\n",
        "\n",
        "**루브릭 평가기준**\n",
        "1. 데이터의 전처리 및 구성과정이 체계적으로 진행되었는가? - 특수문자 제거, 토크나이저 생성, 패딩 처리의 작업들이 빠짐없이 진행되었는가? \n",
        "\n",
        "2. 가사 텍스트 생성 모델이 정상적으로 동작하는가? - 텍스트 제너레이션 결과로 생성된 문장이 해석 가능한 문장인가?\n",
        "3. 텍스트 생성모델이 안정적으로 학습되었는가? - 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n"
      ],
      "metadata": {
        "id": "-ItlfTm3uWho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBxQPVDV2kD3"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RLhvOYUKePxB"
      },
      "outputs": [],
      "source": [
        "import glob #\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **glob** 함수는 인자로 받은 패턴과 이름이 일치하는 모든 파일과 디렉터리의 리스트를 반환합니다. 패턴을 **\"*\"**로 지적하면 모든 파일과 디렉터리를 볼 수 있다. 또한 현재 경로가 아닌 다른 경로에 대해서도 조회할 수 있습니다."
      ],
      "metadata": {
        "id": "E0ahQFGJTDnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQT25yUNmhUC",
        "outputId": "d3e7d361-ad1d-4c0e-eebb-2e0f981c0448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # 드라이브 마운트\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktqUy3jxmjEM",
        "outputId": "e4bbad25-bc0f-452c-d208-b308165cb4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 228280\n",
            "Examples:\n",
            " ['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.']\n"
          ]
        }
      ],
      "source": [
        "txt_file_path = ('/content/drive/MyDrive/Ex6')\n",
        "txt_list = glob.glob(txt_file_path + \"/*.txt\") # glob을 이용하여 리스트를 불러옴\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOyDYgdP2fGl"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_AbznZlUyKEu"
      },
      "outputs": [],
      "source": [
        "import re #\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **import re** 는 정규표현 패턴를 이용한 문자열의 추출이나, 치환, 분할 등이 가능하다. "
      ],
      "metadata": {
        "id": "q7kF-TZgT8WE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6ywQXaRyKSk",
        "outputId": "b9f8ab0d-7e65-4f5f-e274-87f76113c680"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> before we proceed any further , hear me speak . <end>',\n",
              " '<start> speak , speak . <end>',\n",
              " '<start> you are all resolved rather to die than to famish <end>',\n",
              " '<start> resolved . resolved . <end>',\n",
              " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
              " '<start> we know t , we know t . <end>',\n",
              " '<start> is t a verdict <end>',\n",
              " '<start> no more talking on t let it be done away , away ! <end>',\n",
              " '<start> one word , good citizens . <end>',\n",
              " '<start> we are accounted poor citizens , the patricians good . <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "def preprocess_sentence(sentence): \n",
        "    # 입력된 문장을 re 모듈에 있는 re.lower/ re.sub를 활용하여 전처리 한다.\n",
        "    sentence = sentence.lower().strip() #                     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #     2. 특수문자 양쪽에 공백을 넣고\n",
        "    sentence = re.sub(r'(\" \")+', \" \", sentence) #             3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "    sentence = re.sub(r\"[^a-zA-Z.!,¿]+\", \" \", sentence) #    4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "    sentence = sentence.strip() #                             5. 다시 양쪽 공백을 지웁니다\n",
        "    sentence = \"<start> \" + sentence + \" <end>\" #             6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "    return sentence\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0 : continue # 문장의 길이가 0인 경우 건너뜀\n",
        "    if sentence[-1] == ':': continue # 문장의 마지막이 ':'으로 끝날 경우 건너뜀 -> 화자가 표기된 의미없는 문장이기 때문\n",
        "\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    if len(preprocessed_sentence.split()) > 15: continue # 단어별로 토큰화 되기 때문에 15단어를 넘어가는 문장을 제외 -> 토큰 사이즈를 제한\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15bI9e32wx7",
        "outputId": "db92e0f8-08d6-49cd-8022-b2924711ad59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 162  22 ...   0   0   0]\n",
            " [  2 359   4 ...   0   0   0]\n",
            " [  2   7  68 ...   0   0   0]\n",
            " ...\n",
            " [  2  22  68 ...   0   0   0]\n",
            " [  2  35  23 ...   0   0   0]\n",
            " [  2  22  68 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f4d4cb7ab10>\n",
            "(179960, 15)\n"
          ]
        }
      ],
      "source": [
        "def tokenize(corpus): # 토큰화 함수\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, # 12000단어를 기억할 수 있는 tokenizer 생성 \n",
        "        filters=' ', # re 모듈로 문장 정제를 완료했기에 필터 필요치 않음\n",
        "        oov_token=\"<unk>\" # 8000단어에 포함되지 못한 단어는 '<unk>'로 바꿈\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor(텐서플로우로 전처리 하기 때문에 Tensor로 디렉토리 지정)로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 15로 제한했기에 시퀀스 길이가 15 보다 긴 입력 데이터는 없음\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줌(혹은 <unk>가 출력된다.)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tensor, padding='post')\n",
        "    #post -> 뒤에서 채우기, 기본(pre padding은 앞에서 채우기)\n",
        "\n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "print(tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ABnC90oO25j6"
      },
      "outputs": [],
      "source": [
        "src_input = tensor[:, :-1] #마지막 토큰을 제외하고 학습 데이터(X)로 저장\n",
        "tgt_input = tensor[:, 1:] #시작 토큰을 제외하고 타겟 데이터(y)으로 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영어를 처리하는 nlp 이기 때문에 토큰화로 전처리가 가능하다.(음절이 잘 나뉘어 있어서 그런듯?)\n",
        "- 토큰화로 전처리를 했기 때문에 train과 target 지정이 토큰을 지정하는 것과 같다."
      ],
      "metadata": {
        "id": "JSmjrthzVwYF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "NuD_KcL-3Pyx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=29)\n",
        "# sklearn 라이브러리를 활용하여 데이터를 train과 validation으로 나눠줌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SEORDdSF3Sx1"
      },
      "outputs": [],
      "source": [
        "# 모델 학습 과정에서 데이터를 numpy array 그대로 넣어주지 않고 데이터셋 객체를 만들어 줄 것.\n",
        "\n",
        "BUFFER_SIZE = len(enc_train) # buffer_size는 랜덤하게 지정된 숫자를 가져온다.(여기서는 enc_train의 갯수 만큼)\n",
        "BATCH_SIZE = 128             # 배치 사이즈 설정(128로 지정했으니 buffer_size중 128개를 골라서 한방에 처리.)\n",
        "                             # steps_per_epoch = len(enc_train) 143968 / BATCH_SIZE 128 를 tensorflow가 알아서 해준다\n",
        "                             # 아래의 epoch를 보면 1124r가 나올것이다.(143968/128 = 1124.75)\n",
        "\n",
        "# tokenizer에서 구축한 12000개 + 포함되지 않은 0:<pad> = 12001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듬\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# validation 데이터 또한 tf.data.Dataset객체로 만들어줌\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "dataset_val = dataset.shuffle(len(enc_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(BUFFER_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBcRgGueYnTZ",
        "outputId": "7a8977f5-469e-4713-e031-a9d7c3a5a311"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXJ5csbd3Yvv"
      },
      "source": [
        "# AI 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "4PcGmPPU3WC3"
      },
      "outputs": [],
      "source": [
        "#클래스로 문자 생성 함수를 작성\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # 임베딩 레이어는 입력 텐서의 단어 사전 인덱스 값을 이에 해당하는 워드 벡터로 바꿔주는 역할\n",
        "\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True) #\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True) # 자연어 처리에서 거의 기본적으로 쓰이는 RNN구조인 LSTMM 레이어를 두 층 추가해줌\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "embedding_size = 256 # 임베딩 레이어에 들어가는 인자. 값이 크면 추상적인 단어나 모호한 단어를 잘 처리하지만,\n",
        "                     # 수가 너무 많으면 지나치게 과몰입해서 문장을 알아볼 수 없게 된다.\n",
        "\n",
        "hidden_size = 1024 # Output과 같이 나오는 출력층이다. 1024개의 RNN함수를 일렬로 묶어준다고 생각하면 편한듯? \n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![0](https://github.com/fmfmsd/Chamchee/blob/main/Exploration/Ex6/hdsize.png?raw=true)\n",
        "\n",
        "---\n",
        "- RNN에서 Output과 Hidden_size의 메커니즘"
      ],
      "metadata": {
        "id": "Gm8mf4RRbZ4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N0sRdZV3b-L",
        "outputId": "b1d5cfab-7c4f-47ed-b25c-6be64c516860"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-2.8737276e-04,  1.2905178e-04,  1.1627367e-05, ...,\n",
              "         -2.7051248e-05, -1.6505999e-04, -4.6462883e-05],\n",
              "        [-3.8133989e-04,  2.0119951e-04,  1.1872656e-04, ...,\n",
              "         -1.2501820e-04, -3.6328362e-04, -5.2209289e-06],\n",
              "        [-6.2826084e-04,  4.0879159e-04,  2.9453475e-04, ...,\n",
              "         -4.7382896e-04, -5.0596421e-04,  8.7410335e-05],\n",
              "        ...,\n",
              "        [ 3.7495926e-04,  1.3284208e-03, -9.6289434e-05, ...,\n",
              "          1.5473333e-03, -3.9102426e-05, -2.3800023e-04],\n",
              "        [ 3.3947214e-04,  1.0695108e-03, -1.3785916e-04, ...,\n",
              "          1.7255514e-03,  8.7009139e-05, -2.1553837e-04],\n",
              "        [ 5.1098148e-04,  7.7316887e-04, -2.7818064e-04, ...,\n",
              "          1.6299953e-03,  2.3626476e-04, -1.8985358e-04]],\n",
              "\n",
              "       [[-2.8737276e-04,  1.2905178e-04,  1.1627367e-05, ...,\n",
              "         -2.7051248e-05, -1.6505999e-04, -4.6462883e-05],\n",
              "        [-6.5337139e-04,  1.3823550e-04,  4.9793867e-05, ...,\n",
              "         -5.1613897e-05,  8.3270781e-05,  2.2676274e-04],\n",
              "        [-7.9383765e-04,  7.4919684e-05, -1.4131081e-04, ...,\n",
              "         -3.1433118e-04,  2.7902564e-04,  3.0202005e-04],\n",
              "        ...,\n",
              "        [ 9.5054472e-04, -7.4651593e-04, -7.5819402e-04, ...,\n",
              "         -1.1862593e-03,  1.5673254e-03,  7.1448600e-04],\n",
              "        [ 1.4196113e-03, -8.1773545e-04, -8.9615455e-04, ...,\n",
              "         -1.2817540e-03,  1.4591517e-03,  6.1268592e-04],\n",
              "        [ 1.8852564e-03, -8.6613768e-04, -1.0275953e-03, ...,\n",
              "         -1.3692230e-03,  1.2976384e-03,  4.9854076e-04]],\n",
              "\n",
              "       [[-2.8737276e-04,  1.2905178e-04,  1.1627367e-05, ...,\n",
              "         -2.7051248e-05, -1.6505999e-04, -4.6462883e-05],\n",
              "        [-3.3683880e-04, -4.1413801e-05,  1.4311186e-04, ...,\n",
              "          6.3038875e-05, -2.4723797e-04,  7.2994619e-05],\n",
              "        [-2.6776461e-04, -3.7135812e-04,  3.7509663e-04, ...,\n",
              "          2.0077330e-04, -2.8854058e-04,  2.6815140e-04],\n",
              "        ...,\n",
              "        [-4.4787140e-04,  1.1824380e-03,  6.1141670e-04, ...,\n",
              "         -6.7304427e-07, -1.1040199e-03,  1.3309504e-03],\n",
              "        [-6.4705254e-04,  1.2112750e-03,  4.4404942e-04, ...,\n",
              "          3.4670273e-04, -1.3035506e-03,  1.4379980e-03],\n",
              "        [-8.0349745e-04,  1.0276645e-03,  3.8584691e-04, ...,\n",
              "          5.4986437e-04, -1.2102510e-03,  1.6023283e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-2.8737276e-04,  1.2905178e-04,  1.1627367e-05, ...,\n",
              "         -2.7051248e-05, -1.6505999e-04, -4.6462883e-05],\n",
              "        [-3.4688806e-04,  1.8758628e-04, -1.6600725e-05, ...,\n",
              "         -4.6521342e-05, -2.5005455e-04, -1.5437939e-04],\n",
              "        [-4.8687111e-04,  8.2800572e-05,  5.8121735e-05, ...,\n",
              "          3.5405072e-05, -1.5663347e-04, -3.1390315e-05],\n",
              "        ...,\n",
              "        [ 3.1284557e-03, -6.7345297e-04, -1.1053346e-03, ...,\n",
              "         -1.5055320e-03, -2.1659085e-05,  1.2074245e-04],\n",
              "        [ 3.5001561e-03, -7.0964190e-04, -1.2307991e-03, ...,\n",
              "         -1.5888230e-03, -1.4274886e-04,  2.7480863e-05],\n",
              "        [ 3.8300217e-03, -7.3891308e-04, -1.3441863e-03, ...,\n",
              "         -1.6514474e-03, -2.6271475e-04, -6.7464134e-05]],\n",
              "\n",
              "       [[-2.8737276e-04,  1.2905178e-04,  1.1627367e-05, ...,\n",
              "         -2.7051248e-05, -1.6505999e-04, -4.6462883e-05],\n",
              "        [-3.5783052e-04, -2.6306163e-05,  2.0733751e-04, ...,\n",
              "         -1.6331284e-04, -1.1192540e-04,  9.6079981e-05],\n",
              "        [-2.5637061e-04, -2.5134796e-05,  4.0863027e-04, ...,\n",
              "         -1.3537714e-04, -1.2401397e-04, -1.3325851e-04],\n",
              "        ...,\n",
              "        [ 1.7877870e-03, -1.1443523e-03, -5.7680486e-04, ...,\n",
              "         -1.1639491e-03,  9.9211966e-04, -1.5617521e-04],\n",
              "        [ 2.2223277e-03, -1.1554395e-03, -7.6762307e-04, ...,\n",
              "         -1.3318479e-03,  9.3185401e-04, -1.6191392e-04],\n",
              "        [ 2.6401894e-03, -1.1514960e-03, -9.3450211e-04, ...,\n",
              "         -1.4697730e-03,  8.2491612e-04, -1.7870212e-04]],\n",
              "\n",
              "       [[-2.8737276e-04,  1.2905178e-04,  1.1627367e-05, ...,\n",
              "         -2.7051248e-05, -1.6505999e-04, -4.6462883e-05],\n",
              "        [-2.4705229e-04,  1.5863300e-04,  9.2518305e-05, ...,\n",
              "         -4.9705985e-05, -2.6184515e-04, -2.0995236e-04],\n",
              "        [-8.3179679e-05,  6.0195711e-05,  3.7489543e-05, ...,\n",
              "         -1.5646839e-04, -5.4312329e-04, -2.1636249e-04],\n",
              "        ...,\n",
              "        [ 2.9387153e-05, -1.5752908e-04,  6.8781921e-04, ...,\n",
              "         -4.9137077e-05, -1.0099014e-03, -7.3744181e-05],\n",
              "        [ 2.0242199e-04,  1.0145885e-04,  5.8756472e-04, ...,\n",
              "          8.7347857e-05, -1.0837023e-03,  5.2279349e-05],\n",
              "        [ 1.7346583e-04,  1.5223825e-04,  5.5252807e-04, ...,\n",
              "          1.7959776e-04, -9.0840313e-04,  2.2000681e-04]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# 데이터셋에서 데이터 배치 하나만 따로 불러오는 방법\n",
        "for src_sample, tgt_sample in dataset.take(1): break # .take와 브레이크를 같이 써서 하나를 불러온 다음 멈춘다.\n",
        "\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shape=(128, 14, 12001)**\n",
        "\n",
        "|batch_size(배치)|preprocess_sentence>15(최대 문자열)|tokenizer.num_words + 1(토큰개수)|\n",
        "|:---:|:---:|:---:|\n",
        "|128개|14|12000+1개|"
      ],
      "metadata": {
        "id": "RUugpEZScCwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5EtS2523h4Y",
        "outputId": "c4a45735-e1e4-4a71-9357-866e4b2cea19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  3072256   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  12301025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 모델 확인\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERwCOwd-3p09",
        "outputId": "83c28261-3d9b-4c11-cdc8-8e63f2b0164b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1124/1124 [==============================] - 21s 17ms/step - loss: 3.3627 - val_loss: 3.0417\n",
            "Epoch 2/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.9591 - val_loss: 2.8143\n",
            "Epoch 3/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.7763 - val_loss: 2.6513\n",
            "Epoch 4/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.6325 - val_loss: 2.5138\n",
            "Epoch 5/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.5054 - val_loss: 2.3837\n",
            "Epoch 6/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.3881 - val_loss: 2.2690\n",
            "Epoch 7/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.2770 - val_loss: 2.1567\n",
            "Epoch 8/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.1733 - val_loss: 2.0529\n",
            "Epoch 9/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 2.0748 - val_loss: 1.9535\n",
            "Epoch 10/10\n",
            "1124/1124 [==============================] - 18s 16ms/step - loss: 1.9818 - val_loss: 1.8636\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam() # 옵티마이저는 Adam으로 설정\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none') # loss함수\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer) # 10번 Epoch\n",
        "history1 = model.fit(dataset,\n",
        "                    epochs=10,\n",
        "                   validation_data=dataset_val,\n",
        "                   verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**val_loss 루브릭기준에 만족하는 약 1.9가 나왔다.**"
      ],
      "metadata": {
        "id": "_4uAvJUyryrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0LkG4_0y57QZ"
      },
      "outputs": [],
      "source": [
        "# 테스트를 위해서 입력받은 init_sentence를 텐서로 변환\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    \n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듬\n",
        "    while True:\n",
        "        predict = model(test_tensor) # 입력받은 문장의 텐서를 입력합니다\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  # 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # softmax 함수에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "        if predict_word.numpy()[0] == end_token: break # 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "        if test_tensor.shape[1] >= max_len: break \n",
        "\n",
        "    generated = \"\"\n",
        "\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \" # 문장 구조 표현 함수\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xwO7Ncp-5-9z",
        "outputId": "5b57670b-9956-4589-f169-00693fb02cfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i was born to make you happy <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> I was \", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVwkBEXwK8mw"
      },
      "source": [
        "- <  start  > 나는 너를 행복하게 해주기 위해 태어났어. < end > 가 출력 된 것을 확인 (브리티니 스피어스 - Born To Make You Happy가사 https://youtu.be/Yy5cKX4jBkQ?t=121)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "sOr-llJr6B6_"
      },
      "outputs": [],
      "source": [
        "word1 = generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=15)\n",
        "word2 = generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=15)\n",
        "word3 = generate_text(model, tokenizer, init_sentence=\"<start> he\", max_len=15)\n",
        "word4 = generate_text(model, tokenizer, init_sentence=\"<start> she\", max_len=15)\n",
        "word5 = generate_text(model, tokenizer, init_sentence=\"<start> be\", max_len=15)\n",
        "word6 = generate_text(model, tokenizer, init_sentence=\"<start> happy\", max_len=15)\n",
        "word7 = generate_text(model, tokenizer, init_sentence=\"<start> im a\", max_len=15)\n",
        "word8 = generate_text(model, tokenizer, init_sentence=\"<start> His palms\", max_len=15)\n",
        "word9 = generate_text(model, tokenizer, init_sentence=\"<start> gun\", max_len=15)\n",
        "word10 = generate_text(model, tokenizer, init_sentence=\"<start> jail\", max_len=15)\n",
        "word11 = generate_text(model, tokenizer, init_sentence=\"<start> im so high\", max_len=15)\n",
        "word12 = generate_text(model, tokenizer, init_sentence=\"<start> fire\", max_len=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjOQ3opf6GBx",
        "outputId": "be2459a1-dc53-43d6-d7d4-1664343729d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> i m a survivor <end> \n",
            "<start> you know i m a bad boy , i m a cunt <end> \n",
            "<start> he s the lily to the valley , the <unk> <end> \n",
            "<start> she s got me runnin round and round <end> \n",
            "<start> be a little selfish <end> \n",
            "<start> happy birthday , happy birthday , happy birthday woo , shake ! <end> \n",
            "<start> im a bad bad boy <end> \n",
            "<start> his palms are sweaty , knees weak , arms are heavy <end> \n",
            "<start> gun cocking <end> \n",
            "<start> jail , i m a rebel <end> \n",
            "<start> im so high that the ground is gone <end> \n"
          ]
        }
      ],
      "source": [
        "print(word1)\n",
        "print(word2)\n",
        "print(word3)\n",
        "print(word4)\n",
        "print(word5)\n",
        "print(word6)\n",
        "print(word7)\n",
        "print(word8)\n",
        "print(word9)\n",
        "print(word10)\n",
        "print(word11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQZe4jfNK_OA"
      },
      "source": [
        "- word1 - i m a survivor -> 난 살아 남았어.\n",
        "\n",
        "- word2 - you know i m a bad boy , i m a cunt 오...\n",
        "- word3 -> he s the lily to the valley , the <unk>  -> 지미 스와가트의 노래가산데 마지막에 끊김\n",
        "- word4 - she s got me runnin round and round -> Nickelback - Got Me Runnin' Round (Audio) ft. Flo Rida 가사\n",
        "- word5 - be a little selfish -> 좀 이기적으로 살아\n",
        "- word6 -  happy birthday , happy birthday , happy birthday woo , shake ! -> 어떤 노래의 훅 같은 가사가 만들어짐.\n",
        "- word7 - im a bad bad boy -> 난 진짜 나쁜놈이야\n",
        "- word8 - his palms are sweaty , knees weak , arms are heavy -> 에미넴 Lose Yourself의 가사\n",
        "- word9 - gun cocking -> 총 장전소리(https://www.youtube.com/shorts/Er2_owwDS5o)\n",
        "- word10 -  jail , i m a rebel -> 감옥, 나는 반역자다.\n",
        "- word11 - im so high that the ground is gone - 난 너무 높아서 땅이 사라졌어"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **마무리 하며**\n",
        "- 전처리가 제대로 되어서 학습으로 사용한 노래가사를 그대로 출력하는 경우가 나왔다.\n",
        "단 he's / she's 와 같은 축약어의 '가 특수문자로 전처리 되어서 he s / she s 로 음절이 나뉘어서 출력이 된다.\n",
        "- 의외로 cv만큼 재밌었다. 전처리하는 가이드라인이 어느정도 정해져 있어서 그대로 따라 갔을 뿐인데 학습모델이 너무 깔끔하고 자연스럽게 나와서 신기했다."
      ],
      "metadata": {
        "id": "DQvPM3nCstle"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}