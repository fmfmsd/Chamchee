{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBxQPVDV2kD3"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLhvOYUKePxB"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQT25yUNmhUC",
        "outputId": "377489e4-1473-45ba-9bfa-808c14080dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktqUy3jxmjEM",
        "outputId": "b80c344c-8a51-412c-efe5-909ff1062b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 228280\n",
            "Examples:\n",
            " ['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.']\n"
          ]
        }
      ],
      "source": [
        "txt_file_path = ('/content/drive/MyDrive/Ex6')\n",
        "txt_list = glob.glob(txt_file_path + \"/*.txt\")\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "        \n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOyDYgdP2fGl"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AbznZlUyKEu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6ywQXaRyKSk",
        "outputId": "f227059c-a49e-47f5-c110-e18fac76aa54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> before we proceed any further , hear me speak . <end>',\n",
              " '<start> speak , speak . <end>',\n",
              " '<start> you are all resolved rather to die than to famish <end>',\n",
              " '<start> resolved . resolved . <end>',\n",
              " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
              " '<start> we know t , we know t . <end>',\n",
              " '<start> is t a verdict <end>',\n",
              " '<start> no more talking on t let it be done away , away ! <end>',\n",
              " '<start> one word , good citizens . <end>',\n",
              " '<start> we are accounted poor citizens , the patricians good . <end>']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_sentence(sentence): \n",
        "    # 입력된 문장을\n",
        "    sentence = sentence.lower().strip() #                     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #     2. 특수문자 양쪽에 공백을 넣고\n",
        "    sentence = re.sub(r'(\" \")+', \" \", sentence) #             3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "    sentence = re.sub(r\"[^a-zA-Z.!,¿]+\", \" \", sentence) #    4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "    sentence = sentence.strip() #                             5. 다시 양쪽 공백을 지웁니다\n",
        "    sentence = \"<start> \" + sentence + \" <end>\" #             6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "    return sentence\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0 : continue # 문장의 길이가 0인 경우 건너뜀\n",
        "    if sentence[-1] == ':': continue # 문장의 마지막이 ':'으로 끝날 경우 건너뜀 -> 화자가 표기된 의미없는 문장이기 때문\n",
        "        \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    if len(preprocessed_sentence.split()) > 15: continue # 단어별로 토큰화 되기 때문에 15단어를 넘어가는 문장을 제외 -> 토큰 사이즈를 제한\n",
        "    corpus.append(preprocessed_sentence)\n",
        "    \n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15bI9e32wx7",
        "outputId": "ecff3e13-f207-4c2f-a816-6ac79c5de874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  2 162  22 ...   0   0   0]\n",
            " [  2 359   4 ...   0   0   0]\n",
            " [  2   7  68 ...   0   0   0]\n",
            " ...\n",
            " [  2   5  80 ...   3   0   0]\n",
            " [  2  31   5 ...   0   0   0]\n",
            " [  2   1  34 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f28c054fbd0>\n",
            "(179960, 15)\n"
          ]
        }
      ],
      "source": [
        "def tokenize(corpus): # 토큰화 함수\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000,  # 7000단어를 기억할 수 있는 tokenizer 생성 \n",
        "        filters=' ',  # 문장 정제를 완료했기에 필터 필요치 않음\n",
        "        oov_token=\"<unk>\"  # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿈\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 15로 제한했기에 시쿼스 길이가 15 보다 긴 입력 데이터는 없음\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줌\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "print(tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABnC90oO25j6"
      },
      "outputs": [],
      "source": [
        "src_input = tensor[:, :-1] #마지막 토큰을 제외하고 학습 데이터(X)로 저장\n",
        "tgt_input = tensor[:, 1:] #시작 토큰을 제외하고 타겟 데이터(y)으로 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuD_KcL-3Pyx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=29)\n",
        "# sklearn 라이브러리를 활용하여 데이터를 train과 validation으로 나눠줌"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEORDdSF3Sx1"
      },
      "outputs": [],
      "source": [
        "# 모델을 피팅하는 과정에서 데이터를 numpy array 그대로 넣어주지 않고 데이터셋 객체를 만들어줄것\n",
        "# tf.data.Dataset객체를 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능이 제공된다고 함\n",
        "\n",
        "BUFFER_SIZE = len(enc_train) # 전체 데이터수 보다 크거나 같은 값으로 설정해야 모두를 섞을 수 있음\n",
        "BATCH_SIZE = 256 # 배치 사이즈 설정\n",
        "# steps_per_epoch = len(enc_train) // BATCH_SIZE # 한 번의 epoch에 몇번의 스텝이 있는지 설정하는데 어디에 쓰이는지 모르겠음..(???)\n",
        "                                               # 제외하겠음\n",
        "\n",
        "# tokenizer에서 구축한 7000개 + 포함되지 않은 0:<pad> = 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듬\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# validation 데이터 또한 tf.data.Dataset객체로 만들어줌\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "dataset_val = dataset.shuffle(len(enc_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXJ5csbd3Yvv"
      },
      "source": [
        "# AI 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PcGmPPU3WC3"
      },
      "outputs": [],
      "source": [
        "# 모델을 만들어주는 클래스를 만듬\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 임베딩 레이어는 입력 텐서의 단어 사전 인덱스 값을 이에 해당하는 워드 벡터로 바꿔주는 역할\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        # 자연어 처리에서 거의 기본적으로 쓰이는 RNN구조인 LSTMM 레이어를 두 층 추가해줌\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256 # 임베딩 레이어에 들어가는 인자\n",
        "                     # 값이 클수록 단어의 추상적 의미를 잘 파악하지만 데이터가 충분하지 않으면 오히려 혼란의 야기함\n",
        "hidden_size = 1024 # 모델을 처리하는 일꾼의 수라고 이해하면 쉬움\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N0sRdZV3b-L",
        "outputId": "5df16dd9-009d-4c7d-c66b-4ca98ec46f09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
              "array([[[ 9.65514701e-05,  2.27478668e-04, -1.09749672e-04, ...,\n",
              "         -4.77829832e-04,  1.73419085e-05, -2.80712033e-04],\n",
              "        [-3.91485839e-04,  2.49289995e-04, -3.92608228e-04, ...,\n",
              "         -6.97694079e-04,  1.16953843e-05, -1.56791299e-04],\n",
              "        [-7.03829923e-04,  3.32247320e-04, -2.02902273e-04, ...,\n",
              "         -6.65610540e-04, -6.05587957e-05,  3.52315110e-04],\n",
              "        ...,\n",
              "        [ 8.42815149e-04,  1.26152695e-03,  3.73328570e-03, ...,\n",
              "         -3.15485318e-04, -1.91503612e-04,  4.80162562e-04],\n",
              "        [ 1.09656737e-03,  1.23537204e-03,  4.11703438e-03, ...,\n",
              "         -1.81989875e-04, -3.99948593e-04,  5.58939180e-04],\n",
              "        [ 1.33975665e-03,  1.23028096e-03,  4.44319379e-03, ...,\n",
              "         -3.05146023e-05, -5.83747402e-04,  6.22721098e-04]],\n",
              "\n",
              "       [[ 9.65514701e-05,  2.27478668e-04, -1.09749672e-04, ...,\n",
              "         -4.77829832e-04,  1.73419085e-05, -2.80712033e-04],\n",
              "        [ 2.31499318e-04,  2.08288766e-04,  2.18286703e-04, ...,\n",
              "         -5.82095992e-04, -1.91454557e-04, -2.14638683e-04],\n",
              "        [ 2.53605067e-05,  2.98630330e-04,  6.35242686e-05, ...,\n",
              "         -4.00196237e-04, -1.70593150e-04,  1.13830276e-04],\n",
              "        ...,\n",
              "        [ 2.16315020e-04, -6.14769815e-05,  1.45854801e-03, ...,\n",
              "         -4.41709592e-04, -1.26982899e-03,  1.08997081e-03],\n",
              "        [ 5.74868522e-04,  1.35150374e-04,  2.11565150e-03, ...,\n",
              "         -3.51418363e-04, -1.40489952e-03,  1.13057042e-03],\n",
              "        [ 9.13056079e-04,  3.29549308e-04,  2.71771080e-03, ...,\n",
              "         -2.41487971e-04, -1.50441995e-03,  1.15379109e-03]],\n",
              "\n",
              "       [[ 9.65514701e-05,  2.27478668e-04, -1.09749672e-04, ...,\n",
              "         -4.77829832e-04,  1.73419085e-05, -2.80712033e-04],\n",
              "        [-1.41075740e-04,  2.17948778e-04, -3.48189904e-04, ...,\n",
              "         -7.74181914e-04,  6.41893304e-04, -3.83705948e-04],\n",
              "        [-8.10710073e-04,  2.30283957e-04, -4.81312920e-04, ...,\n",
              "         -1.03693840e-03,  3.39019985e-04, -7.11754081e-04],\n",
              "        ...,\n",
              "        [ 8.36232095e-04,  1.06530171e-03,  3.07663344e-03, ...,\n",
              "          2.17545836e-04, -3.23893852e-04, -1.56651382e-04],\n",
              "        [ 1.13189302e-03,  1.09757145e-03,  3.53591912e-03, ...,\n",
              "          3.01685563e-04, -5.19634050e-04,  1.61839143e-05],\n",
              "        [ 1.40412082e-03,  1.13854487e-03,  3.93074052e-03, ...,\n",
              "          3.99912999e-04, -6.97657291e-04,  1.68845407e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 9.65514701e-05,  2.27478668e-04, -1.09749672e-04, ...,\n",
              "         -4.77829832e-04,  1.73419085e-05, -2.80712033e-04],\n",
              "        [ 3.00438784e-04,  5.34896506e-04, -2.09588208e-04, ...,\n",
              "         -3.52940580e-04,  1.22834637e-04, -6.37127378e-04],\n",
              "        [ 2.62259273e-05,  5.44229813e-04, -5.37950546e-04, ...,\n",
              "         -2.09527527e-04,  6.31556832e-05, -5.47783275e-04],\n",
              "        ...,\n",
              "        [ 2.83488480e-04,  7.77114590e-04,  1.01526314e-03, ...,\n",
              "          1.26685144e-03,  3.63269064e-04, -1.04619411e-03],\n",
              "        [ 5.97724400e-04,  7.33009831e-04,  1.54824043e-03, ...,\n",
              "          1.06549473e-03,  1.59824325e-04, -9.57684708e-04],\n",
              "        [ 8.99120932e-04,  7.04733829e-04,  2.09708023e-03, ...,\n",
              "          8.88711773e-04, -7.02631514e-05, -8.29984376e-04]],\n",
              "\n",
              "       [[ 9.65514701e-05,  2.27478668e-04, -1.09749672e-04, ...,\n",
              "         -4.77829832e-04,  1.73419085e-05, -2.80712033e-04],\n",
              "        [-2.61620240e-04,  2.26296790e-04, -4.50351043e-04, ...,\n",
              "         -6.59008103e-04, -8.45430477e-05, -2.23332172e-04],\n",
              "        [-4.71254549e-04,  2.66656833e-04, -3.93734983e-04, ...,\n",
              "         -7.74561195e-04, -7.36342685e-04, -3.54541495e-04],\n",
              "        ...,\n",
              "        [ 1.85106415e-03,  1.70820567e-03,  1.68567151e-03, ...,\n",
              "          1.08642830e-03,  5.44354029e-04,  2.13504201e-04],\n",
              "        [ 2.00779503e-03,  1.62266917e-03,  2.27133278e-03, ...,\n",
              "          1.05581374e-03,  2.99053761e-04,  3.91583482e-04],\n",
              "        [ 2.14721146e-03,  1.55801733e-03,  2.81850412e-03, ...,\n",
              "          1.03860442e-03,  5.13089508e-05,  5.35314670e-04]],\n",
              "\n",
              "       [[ 9.65514701e-05,  2.27478668e-04, -1.09749672e-04, ...,\n",
              "         -4.77829832e-04,  1.73419085e-05, -2.80712033e-04],\n",
              "        [-7.68342943e-05,  4.67613398e-04, -3.63328618e-05, ...,\n",
              "         -6.29976334e-04,  1.35068127e-04, -1.16617965e-04],\n",
              "        [-2.82420951e-04,  6.31701609e-04,  4.52102067e-05, ...,\n",
              "         -5.54397353e-04,  3.72318929e-04,  2.65601993e-04],\n",
              "        ...,\n",
              "        [ 1.81459088e-03,  3.70997062e-04,  2.50359485e-03, ...,\n",
              "          8.58611311e-04,  5.75943792e-04,  1.52381428e-03],\n",
              "        [ 2.02609948e-03,  4.58517898e-04,  3.00669414e-03, ...,\n",
              "          8.63969733e-04,  2.52114114e-04,  1.57011300e-03],\n",
              "        [ 2.20789900e-03,  5.72374905e-04,  3.45272128e-03, ...,\n",
              "          8.81844200e-04, -4.62983262e-05,  1.59480248e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5EtS2523h4Y",
        "outputId": "ecc6d399-cba9-4e1d-b29b-df339b606c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  1792256   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  7176025   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,607,961\n",
            "Trainable params: 22,607,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 모델 확인\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ERwCOwd-3p09",
        "outputId": "20e24786-7c6b-4fa8-c975-37d844c58d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 39/562 [=>............................] - ETA: 1:09 - loss: 0.9103"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-cba20543e839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                    verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam() # 옵티마이저는 Adam으로 설정\n",
        "#Loss\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer) # 30번을 학습\n",
        "history1 = model.fit(dataset,\n",
        "                    epochs=10,\n",
        "                   validation_data=dataset_val,\n",
        "                   verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0LkG4_0y57QZ"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence를 텐서로 변환\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듬\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor)\n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xwO7Ncp-5-9z",
        "outputId": "9c6bc78a-e683-40af-8148-b92fe53d3e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i was playin in the <unk> , the mood all changed <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> I was \", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVwkBEXwK8mw"
      },
      "source": [
        "- 너도 망했어야 해??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "sOr-llJr6B6_"
      },
      "outputs": [],
      "source": [
        "s1 = generate_text(model, tokenizer, init_sentence=\"<start> i m\", max_len=20)\n",
        "s2 = generate_text(model, tokenizer, init_sentence=\"<start> you were\", max_len=20)\n",
        "s3 = generate_text(model, tokenizer, init_sentence=\"<start> he is\", max_len=20)\n",
        "s4 = generate_text(model, tokenizer, init_sentence=\"<start> she is\", max_len=20)\n",
        "s5 = generate_text(model, tokenizer, init_sentence=\"<start> that was\", max_len=20)\n",
        "s6 = generate_text(model, tokenizer, init_sentence=\"<start> there are\", max_len=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjOQ3opf6GBx",
        "outputId": "a1e95984-14fa-4870-8568-d327e3372342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> i m a negative creep <end> \n",
            "<start> you were my first love and you will be my last <end> \n",
            "<start> he is a foreign man <end> \n",
            "<start> she is not fourteen . how long is it now <end> \n",
            "<start> that was the cause of my imprisonment . <end> \n",
            "<start> there are people dying <end> \n"
          ]
        }
      ],
      "source": [
        "print(s1)\n",
        "print(s2)\n",
        "print(s3)\n",
        "print(s4)\n",
        "print(s5)\n",
        "print(s6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQZe4jfNK_OA"
      },
      "source": [
        "1. 나는 부정적인 머저리다.\n",
        "2. 너는 나의 첫사랑이었고 너는 나의 마지막이 될거야.\n",
        "3. 그는 외부인이다.\n",
        "4. 그녀는 14살이 아닙니다. 얼마나 지난거야\n",
        "5. 그것이 나의 투옥의 원인이었다.\n",
        "6. 죽어가는 사람들이 있다\n",
        "\n",
        "---\n",
        "\n",
        "노래 가사를 가져와서 학습을 시키니 뭔가 굉장히 자극적인 것이 나왔다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bBxQPVDV2kD3"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}